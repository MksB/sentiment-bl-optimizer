{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb-ckmWWiKXn"
   },
   "source": [
    "### Notes\n",
    "Diese Implementierung basiert direkt auf **Attilio Meuccis Artikel aus dem Jahr 2008** „The Black-Litterman Approach: Original Model and Extensions” (SSRN 1117574). Jede Formel wurde sorgfältig gemäß der Notation und den Ableitungen des Artikels implementiert.\n",
    "\n",
    "## Mathematische Grundlagen\n",
    "\n",
    "### Wie die Portfolio Optimierung mit dem Black-Litterman Modell gelöste wird\n",
    "Die traditionelle Portfoliooptimierung auf Basis historischer erwarteter Renditen weist folgende Nachteile auf:\n",
    "- **Schätzungsrisiko**: Stichprobenmittelwerte sind äußerst ungenaue Schätzwerte\n",
    "- **Extreme Allokationen**: Geringfügige Änderungen der Eingaben, daraus folgt stark unterschiedliche Portfolios\n",
    "- **Unfähigkeit, Meinungen einzubeziehen**: Keine natürliche Möglichkeit, Marktdaten mit subjektiven Meinungen zu kombinieren\n",
    "\n",
    "\n",
    "Black-Litterman bietet einen Bayes'schen Rahmen, um:\n",
    "1. von einer **stabilen Prior-Verteilung** (oft Gleichgewicht) auszugehen,\n",
    "2. **die Ansichten der Anleger** mit angemessenen Konfidenzniveaus zu berücksichtigen,\n",
    "3. **reibungslose, intuitive Allokationen** zu erzielen.\n",
    "\n",
    "Die Prior-Verteilung\n",
    "\n",
    "Die Marktrenditen werden als normalverteilt modelliert:\n",
    "\n",
    "$$\n",
    "X ~ N(π, Σ)\n",
    "$$\n",
    "\n",
    "Wobei:\n",
    "- **$X$**: Zufallsvektor der Vermögensrenditen (N × 1)\n",
    "- **$\\pi$**: Vorher erwartete Renditen (N × 1)\n",
    "- **$\\Sigma$**: Kovarianzmatrix (N × N)\n",
    "\n",
    "Berechnung von π aus dem Gleichgewicht (CAPM)\n",
    "\n",
    "Die erwarteten Gleichgewichtsrenditen werden aus den Marktgewichten rückberechnet:\n",
    "\n",
    "$$\n",
    "\\pi = 2 \\lambda \\Sigma w_{\\text{eq}}\n",
    "$$\n",
    "\n",
    "Dabei gilt:\n",
    "- **$w_{eq}$**: Marktkapitalisierungsgewichte (N × 1)\n",
    "- **$Λ$** (Lambda): Risikoaversion-Parameter (typischerweise 2-3)\n",
    "- Dies ergibt sich aus der FOC der Mean-Variance-Optimierung: w* = argmax{w'π - λw'Σw}\n",
    "\n",
    "**Implementierung:**\n",
    "```python\n",
    "pi = BlackLittermanModel.compute_equilibrium_returns(w_eq, Sigma, risk_aversion=2.5)\n",
    "```\n",
    "\n",
    "Ansichten und Unsicherheit\n",
    "\n",
    "Ansichten werden als lineare Kombinationen der erwarteten Renditen ausgedrückt:\n",
    "\n",
    "$$\n",
    "P \\pi \\approx N(Q, Ω)\n",
    "$$\n",
    "\n",
    "Dabei gilt:\n",
    "- **P**: Auswahlmatrix (K × N), jede Zeile definiert ein Meinungsportfolio\n",
    "- **Q**: Erwartete Renditen der Meinungen (K × 1)\n",
    "- **$\\Omega$**: Unsicherheitsmatrix der Meinungen (K × K)\n",
    "\n",
    " Erstellen der Auswahlmatrix P\n",
    "\n",
    "**Absolute Sichtweise auf Vermögenswert i:**\n",
    "```\n",
    "P[k,i] = 1,  alle anderen Einträge = 0\n",
    "```\n",
    "Beispiel: „Tech-Aktien werden eine Rendite von 12 % erzielen”\n",
    "```python\n",
    "P = [0, 0, 1, 0]  # Sichtweise auf Vermögenswert 2\n",
    "Q = [0.12]\n",
    "```\n",
    "\n",
    "**Relative Sichtweise (Vermögenswert i übertrifft Vermögenswert j):**\n",
    "```\n",
    "P[k,i] = 1,  P[k,j] = -1,  alle anderen Einträge = 0\n",
    "```\n",
    "Beispiel: „Technologie wird Energie um 5 % übertreffen”\n",
    "```python\n",
    "P = [1, -1, 0, 0]  # Technologie – Energie\n",
    "Q = [0.05]\n",
    "```\n",
    "\n",
    "**Portfolio-Ansicht:**\n",
    "```\n",
    "P[k,:] = Portfolio-Gewichtungen\n",
    "```\n",
    "Beispiel: „Ein gleichgewichtetes Portfolio der ersten beiden Vermögenswerte erzielt eine Rendite von 8 %”\n",
    "```python\n",
    "P = [0,5, 0,5, 0, 0]\n",
    "Q = [0.08]\n",
    "```\n",
    "\n",
    "\n",
    "Angabe der Unsicherheit $\\Omega$\n",
    "\n",
    "**Empfehlung von Meucci (Gleichung 12):**\n",
    "$$\n",
    "\\Omega = (1/c) × P\\Sigma P'\n",
    "$$\n",
    "\n",
    "Dabei gilt:\n",
    "- **c**: Konfidenzparameter (höher = größere Konfidenz in den Ansichten)\n",
    "- Die Struktur übernimmt Korrelationen aus dem Markt $\\Sigma$\n",
    "\n",
    "**Mit relativer Konfidenz (Gleichung 13):**\n",
    "$$\n",
    "\\Omega = (1/c) × diag(u) × P \\Sigma P' × diag(u)\n",
    "$$\n",
    "\n",
    "Wobei:\n",
    "- **u**: Relativer Konfidenzvektor (K × 1)\n",
    "- Höheres $μ[k]$ → GERINGERE Konfidenz in Ansicht k\n",
    "\n",
    "**Implementierung:**\n",
    "```python\n",
    "# Automatische Omega-Berechnung\n",
    "mu_bl, sigma_bl = bl.compute_posterior(P, Q, confidence=2.0)\n",
    "\n",
    "# Manuelle Omega-Spezifikation\n",
    "Omega = np.array([[0.001, 0], [0, 0.002]])  \n",
    "\n",
    "# Unterschiedliche Zuverlässigkeit pro Ansicht\n",
    "mu_bl, sigma_bl = bl.compute_posterior(P, Q, Omega=Omega)\n",
    "```\n",
    "\n",
    "### Posteriorverteilung\n",
    "\n",
    "#### Ursprüngliche Formulierung (Abschnitt 2)\n",
    "\n",
    "Die Ansichten beziehen sich auf den **Parameter** $\\mu$:\n",
    "$$\n",
    "\\mu | Q ; \\Omega \\verb|~| ̃N(\\mu_{BL}, \\Sigma_\\mu)\n",
    "$$\n",
    "\n",
    "Die posterior Marktverteilung lautet:\n",
    "$$\n",
    "X |Q ; \\Omega \\verb|~| N(\\mu_{BL}, \\Sigma_{BL})\n",
    "$$\n",
    "\n",
    "**Wichtige Formeln (Gleichungen 20–21):**\n",
    "$$\n",
    "\\mu_{BL} = \\pi + \\tau \\Sigma P' ( \\tau P \\Sigma P' +  \\Omega)^{-1} (Q-P\\pi)\n",
    "\\\\\n",
    "\\Sigma_{BL} = (1+\\tau) \\Sigma - \\tau^2 \\Sigma P' (\\tau P \\Sigma P' + \\Omega)^{-1} P \\Sigma\n",
    "$$\n",
    "\n",
    "Dabei ist τ die Unsicherheit in der Prior-Verteilung (typischerweise 0,01–0,05).\n",
    "\n",
    "#### Marktformulierung (Abschnitt 3) **[EMPFOHLEN]**\n",
    "\n",
    "Die Ansichten beziehen sich direkt auf die **Marktrenditen** X:\n",
    "```\n",
    "V|x ~ N(Px, Ω)\n",
    "```\n",
    "\n",
    "**Wichtige Formeln (Gleichungen 32-33):**\n",
    "```\n",
    "μ_BL^m = π + ΣP'(PΣP' + Ω)^(-1)(Q - Pπ)\n",
    "\n",
    "Σ_BL^m = Σ - ΣP'(PΣP' + Ω)^(-1)PΣ\n",
    "```\n",
    "\n",
    "**Vorteile der Marktformulierung:**\n",
    "1. **Kein τ-Parameter erforderlich** im Posterior\n",
    "2. **Korrektes Grenzverhalten:**\n",
    "   - Ω → ∞ (kein Vertrauen): Posterior = Prior ✓\n",
    "   - Ω → 0 (volles Vertrauen): Posterior = bedingte Verteilung ✓\n",
    "3. **Natürliche Integration der Szenarioanalyse**\n",
    "\n",
    "**Implementierung:**\n",
    "```python\n",
    "bl = BlackLittermanModel(pi, Sigma, use_market_formulation=True)  # Empfohlen\n",
    "mu_bl, sigma_bl = bl.compute_posterior(P, Q, confidence=2.0)\n",
    "```\n",
    "\n",
    "### Grenzfälle\n",
    "\n",
    "#### Kein Vertrauen (Ω → ∞)\n",
    "\n",
    "Die A-posteriori-Wahrscheinlichkeit sollte der A-priori-Wahrscheinlichkeit entsprechen:\n",
    "```\n",
    "X|Q → X  als Ω → ∞\n",
    "```\n",
    "\n",
    "**Marktformulierung:** ✓ Erreicht dies genau.\n",
    "**Ursprüngliche Formulierung:** A-posteriori = N(π, (1+τ)Σ) ≠ A-priori genau.\n",
    "\n",
    "**Test:**\n",
    "```python\n",
    "mu_bl, _ = bl.compute_posterior(P, Q, confidence=0.001)  # Sehr niedrig.\n",
    "# mu_bl ≈ pi\n",
    "```\n",
    "\n",
    "#### Volles Vertrauen (Ω → 0)\n",
    "\n",
    "Die Posterior-Verteilung sollte der **bedingten Verteilung** entsprechen (Szenarioanalyse):\n",
    "```\n",
    "X|Q → N(μ|Q, Σ|Q)  wenn Ω → 0\n",
    "```\n",
    "\n",
    "Wobei (Gleichungen 25-26):\n",
    "```\n",
    "μ|Q = π + ΣP'(PΣP')^(-1)(Q - Pπ)\n",
    "Σ|Q = Σ - ΣP'(PΣP')^(-1)PΣ\n",
    "```\n",
    "\n",
    "**Marktformulierung:** ✓ Erreicht dies genau\n",
    "**Ursprüngliche Formulierung:** Erreicht nur den bedingten Mittelwert, nicht die Kovarianz\n",
    "\n",
    "**Test:**\n",
    "```python\n",
    "mu_bl, _ = bl.compute_posterior(P, Q, confidence=10000.0)  # Sehr hoch\n",
    "# Für Ansichten Q = [0,10] zu Vermögenswert 0: mu_bl[0] ≈ 0,10\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Implementierungsdetails\n",
    "\n",
    "### Kernalgorithmus\n",
    "\n",
    "```python\n",
    "def compute_posterior_market(P, Q, Omega, pi, Sigma):\n",
    "    „“\"\n",
    "    Implementiert die Gleichungen (32) und (33) von Meucci.\n",
    "    „“\"\n",
    "    # Zwischenberechnung: PΣP' + Ω\n",
    "    M = P @ Sigma @ P.T + Omega\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    \n",
    "    # Gleichung (32): Posterior-Mittelwert\n",
    "    view_adjustment = Q - P @ pi\n",
    "    mu_BL = pi + Sigma @ P.T @ M_inv @ view_adjustment\n",
    "    \n",
    "    # Gleichung (33): Posterior-Kovarianz\n",
    "    Sigma_BL = Sigma - Sigma @ P.T @ M_inv @ P @ Sigma\n",
    "    \n",
    "    return mu_BL, Sigma_BL\n",
    "```\n",
    "\n",
    "### Numerische Stabilität\n",
    "\n",
    "**Herausforderungen:**\n",
    "1. Matrixinversion von M = PΣP' + Ω\n",
    "2. Berechnung von Sigma_BL (Differenz der Matrizen)\n",
    "\n",
    "**Implementierte Lösungen:**\n",
    "\n",
    "```python\n",
    "# Verwendung der Pseudoinverse für nahezu singuläres M\n",
    "try:\n",
    "    M_inv = np.linalg.inv(M)\n",
    "except np.linalg.LinAlgError:\n",
    "    M_inv = np.linalg.pinv(M)  # Stabiler\n",
    "\n",
    "# Sicherstellung der Symmetrie von Sigma_BL\n",
    "Sigma_BL = (Sigma_BL + Sigma_BL.T) / 2\n",
    "```\n",
    "\n",
    "### Validierung\n",
    "Validieren Sie immer die posteriore Kovarianz:\n",
    "\n",
    "```python\n",
    "validation = bl.validate_posterior(sigma_bl)\n",
    "\n",
    "# Überprüfen:\n",
    "assert validation[‚is_symmetric‘]\n",
    "assert validation[‚is_psd‘]  # Positiv semidefinit\n",
    "assert validation[‚condition_number‘] < 1e10  # Gut konditioniert\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Praktischer Leitfaden zur Anwendung\n",
    "\n",
    "### Arbeitsablauf 1: Gleichgewichtsbasiert (CAPM)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from bl_model import BlackLittermanModel\n",
    "\n",
    "# Schritt 1: Schätzen Sie die Kovarianz aus historischen Daten\n",
    "# (unter Verwendung von exponentieller Glättung usw.)\n",
    "Sigma = estimate_covariance(returns_data)\n",
    "\n",
    "# Schritt 2: Marktkapitalisierungsgewichte abrufen\n",
    "w_eq = get_market_weights()  # z. B. [0,60, 0,25, 0,10, 0,05]\n",
    "\n",
    "# Schritt 3: Gleichgewichtsrenditen berechnen\n",
    "pi = BlackLittermanModel.compute_equilibrium_returns(\n",
    "    w_eq, Sigma, risk_aversion=2.5)\n",
    "\n",
    "\n",
    "# Schritt 4: Modell initialisieren\n",
    "bl = BlackLittermanModel(pi, Sigma, use_market_formulation=True)\n",
    "\n",
    "# Schritt 5: Ansichten ausdrücken\n",
    "P = create_view_matrix(\n",
    "    n_assets=4,\n",
    "    absolute_views={0: 1.0},           # US-Aktien = 10 %\n",
    "    relative_views=[(1, 2, 1.0, -1.0)] # Europa – Asien = 2 %\n",
    ")\n",
    "Q = np.array([0.10, 0.02])\n",
    "\n",
    "# Schritt 6: Posterior berechnen\n",
    "mu_BL, Sigma_BL = bl.compute_posterior(P, Q, confidence=2.0)\n",
    "\n",
    "# Schritt 7: Portfolio optimieren\n",
    "w_optimal = mean_variance_optimize(mu_BL, Sigma_BL, risk_aversion=2.5)\n",
    "```\n",
    "\n",
    "\n",
    "### Workflow 2: Benutzerdefinierte Priorität\n",
    "\n",
    "```python\n",
    "# Verwenden Sie Ihr eigenes Modell für pi (z. B. Faktormodell, ML-Vorhersage)\n",
    "pi = my_expected_returns_model()\n",
    "Sigma = estimate_covariance(returns_data)\n",
    "\n",
    "bl = BlackLittermanModel(pi, Sigma, use_market_formulation=True)\n",
    "\n",
    "# Ansichten als Anpassungen Ihres Modells ausdrücken\n",
    "P = create_view_matrix(n_assets=5, relative_views=[(0, 1, 1.0, -1.0)])\n",
    "Q = np.array([0.03])  # „Ich denke, dass Asset 0 Asset 1 um 3 % übertreffen wird.“\n",
    "\n",
    "mu_BL, Sigma_BL = bl.compute_posterior(P, Q, confidence=1.5)\n",
    "```\n",
    "\n",
    "### Workflow 3: Szenarioanalyse\n",
    "\n",
    "```python\n",
    "# Stresstest: Was passiert, wenn Tech um 30 % einbricht?\n",
    "P = create_view_matrix(n_assets=10, absolute_views={tech_idx: 1.0})\n",
    "Q = np.array([-0.30])\n",
    "\n",
    "# Sehr hohe Konfidenz ≈ deterministisches Szenario\n",
    "mu_stress, Sigma_stress = bl.compute_posterior(P, Q, confidence=10000.0)\n",
    "\n",
    "# Analyse der Auswirkungen auf Ihr Portfolio\n",
    "portfolio_return_stress = w_current @ mu_stress\n",
    "```\n",
    "\n",
    "\n",
    "Workflow 2: Custom Priority\n",
    "\n",
    "```python\n",
    "# Use your own model for pi (e.g., factor model, ML prediction)\n",
    "pi = my_expected_returns_model()\n",
    "Sigma = estimate_covariance(returns_data)\n",
    "\n",
    "bl = BlackLittermanModel(pi, Sigma, use_market_formulation=True)\n",
    "\n",
    "# Express views as adjustments to your model\n",
    "P = create_view_matrix(n_assets=5, relative_views=[(0, 1, 1.0, -1.0)])\n",
    "Q = np.array([0.03])  # “I think asset 0 will outperform asset 1 by 3%.”\n",
    "\n",
    "mu_BL, Sigma_BL = bl.compute_posterior(P, Q, confidence=1.5)\n",
    "```\n",
    "\n",
    "### Workflow 3: Scenario analysis\n",
    "\n",
    "```python\n",
    "# Stress test: What happens if Tech slumps by 30%?\n",
    "P = create_view_matrix(n_assets=10, absolute_views={tech_idx: 1.0})\n",
    "Q = np.array([-0.30])\n",
    "\n",
    "# Very high confidence ≈ deterministic scenario\n",
    "mu_stress, Sigma_stress = bl.compute_posterior(P, Q, confidence=10000.0)\n",
    "\n",
    "# Analysis of the impact on your portfolio\n",
    "portfolio_return_stress = w_current @ mu_stress\n",
    "```\n",
    "\n",
    "## Fortgeschrittene Themen\n",
    "\n",
    "### Mehrere Benutzer / Hierarchische Ansichten\n",
    "\n",
    "Zum Kombinieren von Ansichten aus mehreren Quellen (z. B. Branchenanalysten + Makroteam):\n",
    "\n",
    "```python\n",
    "# Analyst 1: Hohes Vertrauen in Ansicht 1\n",
    "P1 = np.array([[1, 0, 0, 0]])\n",
    "Q1 = np.array([0,12])\n",
    "relative_conf1 = np.array([1,0])  # Hohes Vertrauen\n",
    "\n",
    "# Analyst 2: Geringe Zuversicht in Ansicht 2\n",
    "P2 = np.array([[0, 1, 0, 0]])\n",
    "Q2 = np.array([0,08])\n",
    "relative_conf2 = np.array([3,0])  # Geringe Zuversicht (höherer Wert)\n",
    "\n",
    "# Ansichten kombinieren\n",
    "P = np.vstack([P1, P2])\n",
    "Q = np.concatenate([Q1, Q2])\n",
    "relative_conf = np.concatenate([relative_conf1, relative_conf2])\n",
    "\n",
    "mu_bl, _ = bl.compute_posterior(\n",
    "    P, Q,\n",
    "    confidence=2.0,\n",
    "    relative_confidence=relative_conf)\n",
    "\n",
    "```\n",
    "\n",
    "### Nichtlineare Ansichten\n",
    "\n",
    "Für Ansichten, die hinsichtlich der erwarteten Renditen nicht linear sind (z. B. „die Volatilität wird zunehmen”), benötigen Sie Erweiterungen, die über das Standard-BL hinausgehen:\n",
    "\n",
    "- **Meucci (2009)**: Ansichten zu Risikofaktoren mit nichtlinearer Preisbildung\n",
    "- **Entropy Pooling (Meucci 2008)**: Allgemeinster Rahmen\n",
    "\n",
    "### Stresstests für Korrelationen\n",
    "\n",
    "Um Stresstests für Korrelationen (nicht nur Renditen) durchzuführen, verwenden Sie:\n",
    "\n",
    "```python\n",
    "# Sigma direkt für Korrelationsstress modifizieren\n",
    "Sigma_stressed = stress_correlation(Sigma, asset_i, asset_j, new_corr)\n",
    "\n",
    "# Dann BL mit gestresstem Sigma verwenden\n",
    "bl_stress = BlackLittermanModel(pi, Sigma_stressed)\n",
    "```\n",
    "\n",
    "Oder verwenden Sie die Erweiterung von **Qian und Gorman (2001)** für Ansichten zu Kovarianzen.\n",
    "\n",
    "---\n",
    "\n",
    "## Fehlerbehebung\n",
    "\n",
    "### Problem: Posterior-Renditen ändern sich nicht\n",
    "\n",
    "**Symptome:** `mu_BL ≈ pi` selbst bei starken Ansichten\n",
    "\n",
    "**Ursachen:**\n",
    "1. Zu geringe Konfidenz\n",
    "2. Q ≈ Pπ (Ansicht stimmt mit Prior überein)\n",
    "3. Omega zu groß\n",
    "\n",
    "**Lösungen:**\n",
    "```python\n",
    "# Ansicht vs. Prior überprüfen\n",
    "print(„Ansicht Q:“, Q)\n",
    "print(„Prior Pπ:“, P @ pi)  # Sollte unterschiedlich sein\n",
    "\n",
    "# Konfidenz erhöhen\n",
    "mu_bl, _ = bl.compute_posterior(P, Q, confidence=5.0)  # Höheren Wert versuchen\n",
    "\n",
    "# Omega-Größe überprüfen\n",
    "Omega = bl._compute_omega(P, confidence=1.0)\n",
    "print(„Omega:“, Omega)  # Sollte im Vergleich zu PΣP' angemessen sein\n",
    "\n",
    "### Problem: Posterior-Kovarianz nicht positiv definit\n",
    "\n",
    "**Symptome:** Negative Eigenwerte in `Sigma_BL`\n",
    "\n",
    "**Ursachen:**\n",
    "1. Zu viele Ansichten (K nahe bei N)\n",
    "2. Zu hohe Konfidenz der Ansichten (Ω → 0)\n",
    "3. Numerische Instabilität\n",
    "\n",
    "**Lösungen:**\n",
    "```python\n",
    "# Anzahl der Ansichten reduzieren\n",
    "# ODER Zuverlässigkeit reduzieren\n",
    "mu_bl, sigma_bl = bl.compute_posterior(P, Q, confidence=0.5)\n",
    "\n",
    "# Eigenwerte überprüfen\n",
    "eigenvalues = np.linalg.eigvalsh(sigma_bl)\n",
    "print(„Min. Eigenwert:“, eigenvalues.min())\n",
    "\n",
    "# Bei leicht negativem Wert aufgrund numerischer Werte auf PSD projizieren\n",
    "if eigenvalues.min() < 0:\n",
    "    eigenvalues = np.maximum(eigenvalues, 1e-8)\n",
    "    Q_mat, _ = np.linalg.eigh(sigma_bl)\n",
    "    sigma_bl = Q_mat @ np.diag(eigenvalues) @ Q_mat.T\n",
    "```\n",
    "\n",
    "### Problem: Extreme Allokationen\n",
    "\n",
    "**Symptome:** Die optimalen Gewichte sind extrem (z. B. 200 % in einem Vermögenswert).\n",
    "\n",
    "**Ursachen:**\n",
    "1. Übermäßiges Selbstvertrauen\n",
    "2. Unrealistische Q-Werte\n",
    "3. Die Optimierung ist uneingeschränkt.\n",
    "\n",
    "**Lösungen:**\n",
    "```python\n",
    "# Vertrauen reduzieren\n",
    "mu_bl, sigma_bl = bl.compute_posterior(P, Q, confidence=1.0)\n",
    "\n",
    "# Optimierung mit Einschränkungen versehen\n",
    "w_opt = optimize_with_constraints(\n",
    "    mu_bl, sigma_bl,\n",
    "    bounds=[(0, 0.5) for _ in range(N)],  # Max. 50 % pro Vermögenswert\n",
    "    budget=1.0)\n",
    "\n",
    "\n",
    "# Plausibilitätsprüfung der Q-Werte\n",
    "print(„View returns Q:“, Q * 100, „%“)\n",
    "print(„Sind diese Werte realistisch?“)\n",
    "```\n",
    "\n",
    "### Problem: Original vs Market Give Different Results\n",
    "\n",
    "**This is expected!** They are different models.\n",
    "\n",
    "**Original formulation:**\n",
    "- Has τ parameter (uncertainty in prior)\n",
    "- Posterior covariance = (1+τ)Σ - adjustment\n",
    "- More parameters to calibrate\n",
    "\n",
    "**Market formulation:**\n",
    "- No τ in posterior\n",
    "- Cleaner limiting behavior\n",
    "- Generally recommended\n",
    "\n",
    "**Both are valid** - choose based on your needs.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Primary Reference\n",
    "**Meucci, Attilio (2008).** \"The Black-Litterman Approach: Original Model and Extensions\"  \n",
    "*SSRN Electronic Journal*. https://ssrn.com/abstract=1117574\n",
    "\n",
    "**Key insights:**\n",
    "- Market-based formulation (Section 3)\n",
    "- Qualitative views (Section 2, Equation 11)\n",
    "- Relationship to scenario analysis\n",
    "- Extensions overview (Section 4)\n",
    "\n",
    "### Original Papers\n",
    "**Black, F., and Litterman, R. (1990).** \"Asset Allocation: Combining Investor Views with Market Equilibrium\"  \n",
    "*Goldman Sachs Fixed Income Research*\n",
    "\n",
    "**He, G., and Litterman, R. (2002).** \"The Intuition Behind Black-Litterman Model Portfolios\"\n",
    "\n",
    "### Extensions\n",
    "\n",
    "**Qian, E., and Gorman, S. (2001).** \"Conditional Distribution in Portfolio Theory\"  \n",
    "*Financial Analyst Journal* 57:44-51  \n",
    "→ Views on covariances\n",
    "\n",
    "**Almgren, R., and Chriss, N. (2006).** \"Optimal Portfolios from Ordering Information\"  \n",
    "*Journal of Risk* 9:1-47  \n",
    "→ Ranking views\n",
    "\n",
    "**Meucci, A. (2006).** \"Beyond Black-Litterman: Views on Non-Normal Markets\"  \n",
    "*Risk* 19:114-119  \n",
    "→ COP approach\n",
    "\n",
    "**Meucci, A. (2008).** \"Fully Flexible Views: Theory and Practice\"  \n",
    "*Risk* 21:97-102  \n",
    "→ Entropy pooling (most general)\n",
    "\n",
    "**Meucci, A. (2009).** \"Enhancing the Black-Litterman Approach: Views and Stress-Test on Risk Factors\"  \n",
    "*Journal of Asset Management* 10:89-96  \n",
    "→ Non-linear pricing, derivatives\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848,
     "referenced_widgets": [
      "918b6bf4d6e64bbd9eb0da9807c0a213",
      "21ffa03d3ebe40319dfc906a6ee947be",
      "fb2dd08844b94517a216522c14fd3444",
      "7b7c9a73ced241d28e47e707949b22de",
      "9eae3b2eeb9a4a14a36378c894a87e93",
      "96b27f9365de49cb812ce7496206b709",
      "37e7aad095994a61b43e49d50600d7e5",
      "87a2fdd935234350af52733235044a50",
      "5e1b149e1b01408db6fb4edf9ee2026c",
      "ad9bf851455d40659d19f54f6ec93904",
      "aa07ed1828a443fc83e7d1b9e709cca4"
     ]
    },
    "id": "-iZ8kgtmr47E",
    "outputId": "0af0ad07-56ef-4316-ba87-0bb3cd9936ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BLACK-LITTERMAN + FINBERT PORTFOLIO OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "Mode: LIVE (real NewsAPI + FinBERT)\n",
      "API Key: 1246c60fda...\n",
      "Tickers: XLK, XLE, XLF, XLV, NVDA, AAPL, MSFT, GOOGL, XOM, CVX, GS, JPM, PFE, JNJ\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918b6bf4d6e64bbd9eb0da9807c0a213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: ProsusAI/finbert\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Ticker   Sentiment    Weight     Return\n",
      "------------------------------------------------------------\n",
      "XLK      +0.394           7.7%       7.0%\n",
      "XLE      +0.350           7.2%       5.0%\n",
      "XLF      +0.163           6.7%       4.9%\n",
      "XLV      -0.100           5.0%       3.6%\n",
      "NVDA     +0.272           9.3%       9.0%\n",
      "AAPL     +0.213           8.9%       6.1%\n",
      "MSFT     -0.096           5.0%       3.8%\n",
      "GOOGL    +0.479           8.9%       6.9%\n",
      "XOM      -0.012           5.0%       4.0%\n",
      "CVX      +0.164           7.0%       4.7%\n",
      "GS       +0.149          11.4%       8.8%\n",
      "JPM      +0.007           5.0%       6.7%\n",
      "PFE      -0.285           6.9%       5.7%\n",
      "JNJ      +0.553           5.9%       1.4%\n",
      "------------------------------------------------------------\n",
      "Portfolio Return:   5.9%\n",
      "Portfolio Vol:      9.1%\n",
      "Sharpe Ratio:      0.65\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DONE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "  \"\"\"\n",
    "========================================================================================\n",
    "COMPLETE BLACK-LITTERMAN + FINBERT INTEGRATION - FINAL VERSION\n",
    "Single-File Google Colab Ready Implementation\n",
    "========================================================================================\n",
    "\n",
    "========================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: IMPORTS & SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Check and install dependencies for Google Colab\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    print(\"Installing yfinance...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"yfinance\", \"-q\"])\n",
    "    import yfinance as yf\n",
    "\n",
    "try:\n",
    "    from newsapi import NewsApiClient\n",
    "except ImportError:\n",
    "    print(\"Installing newsapi-python...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"newsapi-python\", \"-q\"])\n",
    "    from newsapi import NewsApiClient\n",
    "\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification\n",
    "    import torch\n",
    "except ImportError:\n",
    "    print(\"Installing transformers and torch...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"torch\", \"-q\"])\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification\n",
    "    import torch\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SentimentData:\n",
    "    \"\"\"Container for sentiment analysis results.\"\"\"\n",
    "    ticker: str\n",
    "    sentiment_mean: float\n",
    "    sentiment_std: float\n",
    "    news_count: int\n",
    "    raw_scores: List[float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BlackLittermanView:\n",
    "    \"\"\"Black-Litterman view specification (P, Q, Ω).\"\"\"\n",
    "    P: np.ndarray\n",
    "    Q: np.ndarray\n",
    "    Omega: np.ndarray\n",
    "    metadata: Dict\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: BLACK-LITTERMAN MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class BlackLittermanModel:\n",
    "    \"\"\"Black-Litterman portfolio optimization (Meucci 2008).\"\"\"\n",
    "\n",
    "    def __init__(self, pi: np.ndarray, sigma: np.ndarray):\n",
    "        \"\"\"Initialize Black-Litterman model.\"\"\"\n",
    "        self.pi = np.asarray(pi).flatten()\n",
    "        self.sigma = np.asarray(sigma)\n",
    "\n",
    "        n_assets = len(self.pi)\n",
    "        if self.sigma.shape != (n_assets, n_assets):\n",
    "            raise ValueError(f\"Sigma shape mismatch\")\n",
    "\n",
    "        if not np.allclose(self.sigma, self.sigma.T):\n",
    "            self.sigma = (self.sigma + self.sigma.T) / 2\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_equilibrium_returns(w_eq: np.ndarray, sigma: np.ndarray,\n",
    "                                    risk_aversion: float = 2.5) -> np.ndarray:\n",
    "        \"\"\" equilibrium returns: π = 2λΣw_eq\"\"\"\n",
    "        return 2 * risk_aversion * sigma @ np.asarray(w_eq).flatten()\n",
    "\n",
    "    def compute_posterior(self, P: np.ndarray, Q: np.ndarray,\n",
    "                         Omega: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Black-Litterman posterior (Meucci 2008, Eq 32-33).\n",
    "\n",
    "        μ_BL = π + ΣP'(PΣP' + Ω)^(-1)(Q - Pπ)\n",
    "        Σ_BL = Σ - ΣP'(PΣP' + Ω)^(-1)PΣ\n",
    "        \"\"\"\n",
    "        P = np.asarray(P)\n",
    "        Q = np.asarray(Q).flatten()\n",
    "        Omega = np.asarray(Omega)\n",
    "\n",
    "        K, N = P.shape\n",
    "        if Omega.ndim == 0 and K == 1:\n",
    "            Omega = Omega.reshape((1, 1))\n",
    "\n",
    "        # M = PΣP' + Ω\n",
    "        M = P @ self.sigma @ P.T + Omega\n",
    "\n",
    "        # Stable inversion\n",
    "        try:\n",
    "            M_inv = np.linalg.inv(M)\n",
    "        except:\n",
    "            M_inv = np.linalg.pinv(M)\n",
    "\n",
    "        # Posterior mean\n",
    "        mu_bl = self.pi + self.sigma @ P.T @ M_inv @ (Q - P @ self.pi)\n",
    "\n",
    "        # Posterior covariance\n",
    "        sigma_bl = self.sigma - self.sigma @ P.T @ M_inv @ P @ self.sigma\n",
    "\n",
    "        return mu_bl, sigma_bl\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: VIEW GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "class ViewGenerator:\n",
    "    \"\"\" Black-Litterman views von FinBERT sentiment.\"\"\"\n",
    "\n",
    "    def __init__(self, tickers: List[str], volatilities: np.ndarray,\n",
    "                 sentiment_scaling: float = 0.02,\n",
    "                 base_uncertainty: float = 0.0001,\n",
    "                 news_volume_weight: float = 0.5,\n",
    "                 consistency_weight: float = 0.5,\n",
    "                 min_news_count: int = 3):\n",
    "        \"\"\"Initialize ViewGenerator.\"\"\"\n",
    "        self.tickers = tickers\n",
    "        self.volatilities = np.asarray(volatilities)\n",
    "        self.N = len(tickers)\n",
    "        self.sentiment_scaling = sentiment_scaling\n",
    "        self.base_uncertainty = base_uncertainty\n",
    "        self.news_volume_weight = news_volume_weight\n",
    "        self.consistency_weight = consistency_weight\n",
    "        self.min_news_count = min_news_count\n",
    "        self.ticker_to_idx = {t: i for i, t in enumerate(tickers)}\n",
    "\n",
    "    def generate_views(self, sentiment_data: Dict[str, SentimentData],\n",
    "                      prior_returns: np.ndarray,\n",
    "                      filter_weak_signals: bool = True,\n",
    "                      min_abs_sentiment: float = 0.1) -> BlackLittermanView:\n",
    "        \"\"\" Black-Litterman views from sentiment data.\"\"\"\n",
    "\n",
    "        # Filter valid views\n",
    "        valid_views = []\n",
    "        for ticker, data in sentiment_data.items():\n",
    "            if ticker not in self.ticker_to_idx:\n",
    "                continue\n",
    "            if data.news_count < self.min_news_count:\n",
    "                continue\n",
    "            if filter_weak_signals and abs(data.sentiment_mean) < min_abs_sentiment:\n",
    "                continue\n",
    "            valid_views.append(data)\n",
    "\n",
    "        if not valid_views:\n",
    "            # Null view\n",
    "            P = np.zeros((1, self.N))\n",
    "            P[0, 0] = 1.0\n",
    "            Q = np.zeros(1)\n",
    "            Omega = np.eye(1) * 1e10\n",
    "            metadata = {'null_view': True, 'tickers': []}\n",
    "            return BlackLittermanView(P, Q, Omega, metadata)\n",
    "\n",
    "        K = len(valid_views)\n",
    "\n",
    "        # Build P\n",
    "        P = np.zeros((K, self.N))\n",
    "        for k, view_data in enumerate(valid_views):\n",
    "            asset_idx = self.ticker_to_idx[view_data.ticker]\n",
    "            P[k, asset_idx] = 1.0\n",
    "\n",
    "        # Compute Q: Q = π + sentiment × σ × scaling\n",
    "        Q = np.zeros(K)\n",
    "        for k, view_data in enumerate(valid_views):\n",
    "            asset_idx = self.ticker_to_idx[view_data.ticker]\n",
    "            base_return = prior_returns[asset_idx]\n",
    "            sentiment_impact = (view_data.sentiment_mean *\n",
    "                              self.volatilities[asset_idx] *\n",
    "                              self.sentiment_scaling)\n",
    "            Q[k] = base_return + sentiment_impact\n",
    "\n",
    "        # Compute Ω\n",
    "        Omega = np.zeros((K, K))\n",
    "        for k, view_data in enumerate(valid_views):\n",
    "            volume_unc = 1.0 / np.sqrt(max(view_data.news_count, 1))\n",
    "            consistency_unc = view_data.sentiment_std ** 2\n",
    "            Omega[k, k] = (self.base_uncertainty +\n",
    "                          self.news_volume_weight * volume_unc +\n",
    "                          self.consistency_weight * consistency_unc)\n",
    "\n",
    "        metadata = {\n",
    "            'tickers': [v.ticker for v in valid_views],\n",
    "            'sentiments': [v.sentiment_mean for v in valid_views],\n",
    "            'news_counts': [v.news_count for v in valid_views]\n",
    "        }\n",
    "\n",
    "        return BlackLittermanView(P, Q, Omega, metadata)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: FINBERT ANALYZER\n",
    "# ============================================================================\n",
    "\n",
    "class FinBERTAnalyzer:\n",
    "    \"\"\"FinBERT sentiment analysis.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Load FinBERT model.\"\"\"\n",
    "        logger.info(\"Loading FinBERT model...\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "        self.model.eval()\n",
    "        logger.info(\"✓ FinBERT loaded\")\n",
    "\n",
    "    def analyze_sentiment(self, texts: List[str]) -> Tuple[np.ndarray, float, float]:\n",
    "        \"\"\"Analyze sentiment: Score = P(Pos) - P(Neg)\"\"\"\n",
    "        if not texts:\n",
    "            return np.array([]), 0.0, 0.5\n",
    "\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True,\n",
    "                               max_length=512, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()\n",
    "        scores = probs[:, 0] - probs[:, 1]  # P(Pos) - P(Neg)\n",
    "\n",
    "        return probs, float(np.mean(scores)), float(np.std(scores)) if len(scores) > 1 else 0.5\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: MAIN OPTIMIZER CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class SentimentBlackLittermanOptimizer:\n",
    "    \"\"\"Complete integration: NewsAPI → FinBERT → Black-Litterman\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, tickers: List[str],\n",
    "                 lookback_days: int = 252,\n",
    "                 news_lookback_days: int = 7,\n",
    "                 articles_per_ticker: int = 10,\n",
    "                 risk_aversion: float = 2.5,\n",
    "                 sentiment_scaling: float = 0.02):\n",
    "        \"\"\"Initialize optimizer.\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.tickers = tickers\n",
    "        self.N = len(tickers)\n",
    "        self.lookback_days = lookback_days\n",
    "        self.news_lookback_days = news_lookback_days\n",
    "        self.articles_per_ticker = articles_per_ticker\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.sentiment_scaling = sentiment_scaling\n",
    "\n",
    "        self.newsapi = NewsApiClient(api_key=api_key)\n",
    "        self.finbert = FinBERTAnalyzer()\n",
    "\n",
    "        self.sigma = None\n",
    "        self.volatilities = None\n",
    "        self.pi = None\n",
    "        self.w_eq = None\n",
    "        self.bl_model = None\n",
    "        self.view_generator = None\n",
    "\n",
    "    def fetch_news(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news for ticker.\"\"\"\n",
    "        try:\n",
    "            to_date = datetime.now()\n",
    "            from_date = to_date - timedelta(days=self.news_lookback_days)\n",
    "\n",
    "            response = self.newsapi.get_everything(\n",
    "                q=ticker,\n",
    "                language='en',\n",
    "                sort_by='relevancy',\n",
    "                page_size=self.articles_per_ticker,\n",
    "                from_param=from_date.strftime('%Y-%m-%d'),\n",
    "                to=to_date.strftime('%Y-%m-%d')\n",
    "            )\n",
    "\n",
    "            articles = response.get('articles', [])\n",
    "            logger.info(f\"  ✓ {ticker}: {len(articles)} articles\")\n",
    "            return articles\n",
    "        except Exception as e:\n",
    "            logger.error(f\"  ✗ {ticker}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def process_ticker(self, ticker: str) -> Optional[SentimentData]:\n",
    "        \"\"\"Fetch news and analyze sentiment.\"\"\"\n",
    "        logger.info(f\"\\nProcessing {ticker}:\")\n",
    "\n",
    "        articles = self.fetch_news(ticker)\n",
    "        if not articles:\n",
    "            return None\n",
    "\n",
    "        texts = []\n",
    "        for article in articles:\n",
    "            title = article.get('title', '')\n",
    "            desc = article.get('description', '')\n",
    "            text = f\"{title}. {desc}\" if desc else title\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "\n",
    "        if not texts:\n",
    "            return None\n",
    "\n",
    "        probs, sentiment_mean, sentiment_std = self.finbert.analyze_sentiment(texts)\n",
    "\n",
    "        logger.info(f\"  Sentiment: {sentiment_mean:+.3f} (std: {sentiment_std:.3f})\")\n",
    "        logger.info(f\"  Sample: {texts[0][:60]}...\")\n",
    "\n",
    "        return SentimentData(\n",
    "            ticker=ticker,\n",
    "            sentiment_mean=sentiment_mean,\n",
    "            sentiment_std=sentiment_std,\n",
    "            news_count=len(texts),\n",
    "            raw_scores=(probs[:, 0] - probs[:, 1]).tolist()\n",
    "        )\n",
    "\n",
    "    def load_market_data(self):\n",
    "        \"\"\"Load historical data - FIXED VERSION.\"\"\"\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(\"LOADING MARKET DATA\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=self.lookback_days + 30)\n",
    "\n",
    "        # Download all tickers at once (more efficient)\n",
    "        logger.info(f\"Downloading data for {len(self.tickers)} tickers...\")\n",
    "\n",
    "        try:\n",
    "            # Download all at once\n",
    "            data = yf.download(self.tickers, start=start_date, end=end_date,\n",
    "                             group_by='ticker', progress=False, threads=True)\n",
    "\n",
    "            # Extract Close prices\n",
    "            prices_dict = {}\n",
    "            for ticker in self.tickers:\n",
    "                try:\n",
    "                    if len(self.tickers) == 1:\n",
    "                        # Single ticker case\n",
    "                        prices_dict[ticker] = data['Close']\n",
    "                    else:\n",
    "                        # Multiple tickers\n",
    "                        prices_dict[ticker] = data[ticker]['Close']\n",
    "\n",
    "                    logger.info(f\"  ✓ {ticker}: {len(prices_dict[ticker])} days\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"  ⚠ {ticker}: {e}\")\n",
    "\n",
    "            if not prices_dict:\n",
    "                raise ValueError(\"No data loaded for any ticker!\")\n",
    "\n",
    "            # Create DataFrame and align\n",
    "            prices_df = pd.DataFrame(prices_dict)\n",
    "            prices_df = prices_df.dropna()\n",
    "\n",
    "            if len(prices_df) < 50:\n",
    "                raise ValueError(f\"Insufficient data: only {len(prices_df)} days\")\n",
    "\n",
    "            logger.info(f\"\\nCommon trading days: {len(prices_df)}\")\n",
    "\n",
    "            # Compute returns\n",
    "            returns_df = np.log(prices_df / prices_df.shift(1)).dropna()\n",
    "\n",
    "            # Estimate covariance\n",
    "            self.sigma = returns_df.cov().values * 252\n",
    "            self.sigma += np.eye(self.N) * 1e-6\n",
    "\n",
    "            # Volatilities\n",
    "            self.volatilities = np.sqrt(np.diag(self.sigma))\n",
    "\n",
    "            logger.info(\"\\nVolatilities (annualized):\")\n",
    "            for i, ticker in enumerate(self.tickers):\n",
    "                logger.info(f\"  {ticker}: {self.volatilities[i]*100:5.1f}%\")\n",
    "\n",
    "            # Equilibrium\n",
    "            self.w_eq = np.ones(self.N) / self.N\n",
    "            self.pi = BlackLittermanModel.compute_equilibrium_returns(\n",
    "                self.w_eq, self.sigma, self.risk_aversion\n",
    "            )\n",
    "\n",
    "            logger.info(\"\\nEquilibrium Returns:\")\n",
    "            for i, ticker in enumerate(self.tickers):\n",
    "                logger.info(f\"  {ticker}: {self.pi[i]*100:5.1f}%\")\n",
    "\n",
    "            # Initialize models\n",
    "            self.bl_model = BlackLittermanModel(self.pi, self.sigma)\n",
    "            self.view_generator = ViewGenerator(\n",
    "                tickers=self.tickers,\n",
    "                volatilities=self.volatilities,\n",
    "                sentiment_scaling=self.sentiment_scaling\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading market data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def optimize(self) -> Optional[Dict]:\n",
    "        \"\"\"Run complete optimization.\"\"\"\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(\"SENTIMENT BLACK-LITTERMAN OPTIMIZATION\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "\n",
    "        # Load market data\n",
    "        if self.sigma is None:\n",
    "            self.load_market_data()\n",
    "\n",
    "        # Fetch news & analyze\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(\"FETCHING NEWS & ANALYZING SENTIMENT\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "\n",
    "        sentiment_data = {}\n",
    "        for ticker in self.tickers:\n",
    "            data = self.process_ticker(ticker)\n",
    "            if data:\n",
    "                sentiment_data[ticker] = data\n",
    "\n",
    "        if not sentiment_data:\n",
    "            logger.error(\"\\n✗ No valid sentiment data\")\n",
    "            return None\n",
    "\n",
    "        # Generate views\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(\"GENERATING VIEWS\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "\n",
    "        view = self.view_generator.generate_views(\n",
    "            sentiment_data, self.pi,\n",
    "            filter_weak_signals=True, min_abs_sentiment=0.10\n",
    "        )\n",
    "\n",
    "        logger.info(f\"\\n✓ Generated {view.P.shape[0]} views\")\n",
    "        logger.info(f\"  Tickers: {view.metadata.get('tickers', [])}\")\n",
    "\n",
    "        # Black-Litterman\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(\"BLACK-LITTERMAN POSTERIOR\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "\n",
    "        mu_bl, sigma_bl = self.bl_model.compute_posterior(view.P, view.Q, view.Omega)\n",
    "\n",
    "        logger.info(\"\\nPosterior Returns:\")\n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "            delta = mu_bl[i] - self.pi[i]\n",
    "            direction = \"↑\" if delta > 0 else \"↓\" if delta < 0 else \"→\"\n",
    "            logger.info(f\"  {ticker}: {self.pi[i]*100:5.1f}% → {mu_bl[i]*100:5.1f}% \"\n",
    "                       f\"{direction} ({delta*100:+.1f}%)\")\n",
    "\n",
    "        # Optimize\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(\"PORTFOLIO OPTIMIZATION\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "\n",
    "        try:\n",
    "            sigma_inv = np.linalg.inv(sigma_bl)\n",
    "        except:\n",
    "            sigma_inv = np.linalg.pinv(sigma_bl)\n",
    "\n",
    "        w_optimal = sigma_inv @ mu_bl / (2 * self.risk_aversion)\n",
    "        w_optimal = np.maximum(w_optimal, 0)\n",
    "        w_optimal = w_optimal / np.sum(w_optimal)\n",
    "\n",
    "        logger.info(\"\\nOptimal Weights:\")\n",
    "        for i, ticker in enumerate(self.tickers):\n",
    "            logger.info(f\"  {ticker}: {w_optimal[i]*100:5.1f}%\")\n",
    "\n",
    "        portfolio_return = w_optimal @ mu_bl\n",
    "        portfolio_vol = np.sqrt(w_optimal @ sigma_bl @ w_optimal)\n",
    "        sharpe = portfolio_return / portfolio_vol if portfolio_vol > 0 else 0\n",
    "\n",
    "        logger.info(f\"\\nPortfolio:\")\n",
    "        logger.info(f\"  Return: {portfolio_return*100:5.1f}%\")\n",
    "        logger.info(f\"  Vol:    {portfolio_vol*100:5.1f}%\")\n",
    "        logger.info(f\"  Sharpe: {sharpe:.2f}\")\n",
    "\n",
    "        return {\n",
    "            'timestamp': datetime.now(),\n",
    "            'tickers': self.tickers,\n",
    "            'sentiment_data': sentiment_data,\n",
    "            'view': view,\n",
    "            'prior_returns': self.pi,\n",
    "            'posterior_returns': mu_bl,\n",
    "            'prior_weights': self.w_eq,\n",
    "            'optimal_weights': w_optimal,\n",
    "            'portfolio_return': portfolio_return,\n",
    "            'portfolio_vol': portfolio_vol,\n",
    "            'sharpe_ratio': sharpe\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: DEMO MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_demo_mode(tickers: List[str] = None):\n",
    "    \"\"\"Demo mode with simulated sentiment (no API needed).\"\"\"\n",
    "    if tickers is None:\n",
    "        tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO MODE - SIMULATED SENTIMENT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load market data\n",
    "    print(\"\\nLoading market data...\")\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=282)\n",
    "\n",
    "    try:\n",
    "        data = yf.download(tickers, start=start_date, end=end_date,\n",
    "                          group_by='ticker', progress=False)\n",
    "\n",
    "        prices_dict = {}\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                if len(tickers) == 1:\n",
    "                    prices_dict[ticker] = data['Close']\n",
    "                else:\n",
    "                    prices_dict[ticker] = data[ticker]['Close']\n",
    "                print(f\"  ✓ {ticker}: {len(prices_dict[ticker])} days\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        prices_df = pd.DataFrame(prices_dict).dropna()\n",
    "        returns_df = np.log(prices_df / prices_df.shift(1)).dropna()\n",
    "\n",
    "        N = len(tickers)\n",
    "        sigma = returns_df.cov().values * 252\n",
    "        sigma += np.eye(N) * 1e-6\n",
    "        volatilities = np.sqrt(np.diag(sigma))\n",
    "\n",
    "        w_eq = np.ones(N) / N\n",
    "        pi = BlackLittermanModel.compute_equilibrium_returns(w_eq, sigma, 2.5)\n",
    "\n",
    "        print(\"\\nEquilibrium Returns:\")\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            print(f\"  {ticker}: {pi[i]*100:5.1f}%\")\n",
    "\n",
    "        # Simulated sentiment\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SIMULATED SENTIMENT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        sentiment_data = {}\n",
    "\n",
    "        base_sentiments = {'AAPL': 0.45, 'MSFT': 0.35, 'GOOGL': 0.25,\n",
    "                          'TSLA': -0.20, 'NVDA': 0.60}\n",
    "\n",
    "        for ticker in tickers:\n",
    "            base = base_sentiments.get(ticker, np.random.uniform(-0.3, 0.3))\n",
    "            news_count = np.random.randint(8, 25)\n",
    "            raw_scores = np.clip(np.random.normal(base, 0.2, news_count), -1, 1)\n",
    "\n",
    "            sentiment_data[ticker] = SentimentData(\n",
    "                ticker=ticker,\n",
    "                sentiment_mean=float(np.mean(raw_scores)),\n",
    "                sentiment_std=float(np.std(raw_scores)),\n",
    "                news_count=news_count,\n",
    "                raw_scores=raw_scores.tolist()\n",
    "            )\n",
    "\n",
    "            print(f\"  {ticker}: {sentiment_data[ticker].sentiment_mean:+.3f} \"\n",
    "                  f\"({news_count} articles)\")\n",
    "\n",
    "        # Generate views\n",
    "        generator = ViewGenerator(tickers, volatilities, 0.02)\n",
    "        view = generator.generate_views(sentiment_data, pi, True, 0.10)\n",
    "\n",
    "        print(f\"\\n✓ Generated {view.P.shape[0]} views\")\n",
    "\n",
    "        # Black-Litterman\n",
    "        bl = BlackLittermanModel(pi, sigma)\n",
    "        mu_bl, sigma_bl = bl.compute_posterior(view.P, view.Q, view.Omega)\n",
    "\n",
    "        # Optimize\n",
    "        sigma_inv = np.linalg.pinv(sigma_bl)\n",
    "        w_optimal = sigma_inv @ mu_bl / 5.0\n",
    "        w_optimal = np.maximum(w_optimal, 0)\n",
    "        w_optimal = w_optimal / np.sum(w_optimal)\n",
    "\n",
    "        # Results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL PORTFOLIO\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\n{'Ticker':<8} {'Sentiment':<12} {'Weight':<10} {'Return'}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            sent = sentiment_data[ticker].sentiment_mean\n",
    "            print(f\"{ticker:<8} {sent:>+.3f}         {w_optimal[i]*100:>5.1f}%     \"\n",
    "                  f\"{mu_bl[i]*100:>5.1f}%\")\n",
    "\n",
    "        portfolio_return = w_optimal @ mu_bl\n",
    "        portfolio_vol = np.sqrt(w_optimal @ sigma_bl @ w_optimal)\n",
    "        sharpe = portfolio_return / portfolio_vol if portfolio_vol > 0 else 0\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Portfolio Return: {portfolio_return*100:>5.1f}%\")\n",
    "        print(f\"Portfolio Vol:    {portfolio_vol*100:>5.1f}%\")\n",
    "        print(f\"Sharpe Ratio:     {sharpe:>5.2f}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        return {'success': True}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 8: MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Google Colab Execution\n",
    "\n",
    "    Two modes:\n",
    "    1. DEMO MODE (simulated sentiment) - No API key needed\n",
    "    2. LIVE MODE (real news) - Uses NewsAPI\n",
    "    \"\"\"\n",
    "\n",
    "    # ========================================================================\n",
    "    # CONFIGURATION\n",
    "    # ========================================================================\n",
    "\n",
    "    USE_DEMO_MODE = False  # Set to True for demo, False for live\n",
    "\n",
    "    # YOUR REAL API KEY\n",
    "    NEWS_API_KEY = \"  \"  #  https://newsapi.org/  kostenloser ApiNews mit email holen\n",
    "\n",
    "    # Portfolio\n",
    "    TICKERS = ['XLK', 'XLE', 'XLF', 'XLV', 'NVDA', 'AAPL', 'MSFT', 'GOOGL', 'XOM','CVX','GS','JPM','PFE','JNJ']\n",
    "    # TICKERS = ['AAPL', 'MSFT', 'GOOGL']  # Start with 3 for testing\n",
    "\n",
    "    # Parameters\n",
    "    LOOKBACK_DAYS = 252\n",
    "    NEWS_LOOKBACK_DAYS = 7\n",
    "    RISK_AVERSION = 2.5\n",
    "\n",
    "    # ========================================================================\n",
    "    # RUN\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BLACK-LITTERMAN + FINBERT PORTFOLIO OPTIMIZATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if USE_DEMO_MODE:\n",
    "        print(\"\\nMode: DEMO (simulated sentiment)\")\n",
    "        print(\"Set USE_DEMO_MODE = False for live news\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        results = run_demo_mode(TICKERS)\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMode: LIVE (real NewsAPI + FinBERT)\")\n",
    "        print(f\"API Key: {NEWS_API_KEY[:10]}...\")\n",
    "        print(f\"Tickers: {', '.join(TICKERS)}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        try:\n",
    "            optimizer = SentimentBlackLittermanOptimizer(\n",
    "                api_key=NEWS_API_KEY,\n",
    "                tickers=TICKERS,\n",
    "                lookback_days=LOOKBACK_DAYS,\n",
    "                news_lookback_days=NEWS_LOOKBACK_DAYS,\n",
    "                risk_aversion=RISK_AVERSION\n",
    "            )\n",
    "\n",
    "            results = optimizer.optimize()\n",
    "\n",
    "            if results:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"✓ OPTIMIZATION COMPLETE\")\n",
    "                print(\"=\"*80)\n",
    "\n",
    "                print(f\"\\n{'Ticker':<8} {'Sentiment':<12} {'Weight':<10} {'Return'}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "                for i, ticker in enumerate(TICKERS):\n",
    "                    sent_data = results['sentiment_data'].get(ticker)\n",
    "                    sent = sent_data.sentiment_mean if sent_data else 0.0\n",
    "                    weight = results['optimal_weights'][i]\n",
    "                    ret = results['posterior_returns'][i]\n",
    "\n",
    "                    print(f\"{ticker:<8} {sent:>+.3f}         {weight*100:>5.1f}%     \"\n",
    "                          f\"{ret*100:>5.1f}%\")\n",
    "\n",
    "                print(\"-\" * 60)\n",
    "                print(f\"Portfolio Return: {results['portfolio_return']*100:>5.1f}%\")\n",
    "                print(f\"Portfolio Vol:    {results['portfolio_vol']*100:>5.1f}%\")\n",
    "                print(f\"Sharpe Ratio:     {results['sharpe_ratio']:>5.2f}\")\n",
    "                print(\"=\"*80)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"✗ ERROR\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"\\nError: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"\\nTry setting USE_DEMO_MODE = True to test without API\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DONE\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}